{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test querying the chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing_extensions import List, TypedDict, Literal\n",
    "from typing import Optional, Annotated\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain_community.vectorstores import AzureSearch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "oai_model = os.getenv(\"AZURE_DEPLOYMENT_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=oai_model,\n",
    "    api_version=\"2024-02-01\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_EMBED_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_EMBED_API_KEY\")\n",
    ")\n",
    "\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=os.getenv(\"AZURE_SEARCH_SERVICE\"),\n",
    "    azure_search_key=os.environ[\"AZURE_SEARCH_API_KEY\"],\n",
    "    index_name=\"dataroots-guidelines-vector-index\",\n",
    "    embedding_function=embeddings.embed_query,\n",
    "    additional_search_client_options={\"retry_total\": 4},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use five sentences maximum. Keep the answer as concise as possible. Always say \"Would you like more information?\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Search(TypedDict):\n",
    "    \"\"\"Search query.\"\"\"\n",
    "\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    section: Annotated[\n",
    "        Optional[str],\n",
    "        ...,\n",
    "        \"Project name from PDF source to filter by.\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query(state: State):\n",
    "    structured_llm = llm.with_structured_output(Search)\n",
    "    query = structured_llm.invoke(state[\"question\"])\n",
    "    return {\"query\": query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    if state.get(\"project\"):\n",
    "        filter = {\"source\": {\"$regex\": state[\"project\"], \"$options\": \"i\"}}\n",
    "        retrieved_docs = vector_store.similarity_search(\n",
    "            state[\"question\"],\n",
    "            filter=filter,\n",
    "            k=10,\n",
    "            search_type=\"hybrid\"\n",
    "        )\n",
    "    else:\n",
    "        retrieved_docs = vector_store.similarity_search(\n",
    "            state[\"question\"],\n",
    "            k=10,\n",
    "            search_type=\"hybrid\"\n",
    "        )\n",
    "    return {\"context\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = custom_rag_prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])\n",
    "graph_builder.add_edge(START, \"analyze_query\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Is there a project about chatbots? If so, what's his title? What are the section titles?\"\n",
    "result = graph.invoke({\"question\": query})\n",
    "\n",
    "print(f'Context: {result[\"context\"]}\\n')\n",
    "print(f'Answer: {result[\"answer\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to memory chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def rag_search(query: str) -> str:\n",
    "    \"\"\"Search through the document database to find relevant information.\"\"\"\n",
    "    # Perform vector search\n",
    "    retrieved_docs = vector_store.similarity_search(\n",
    "        query,\n",
    "        k=10,\n",
    "        search_type=\"hybrid\"\n",
    "    )\n",
    "    \n",
    "    # Format the context with metadata\n",
    "    context_pieces = []\n",
    "    for doc in retrieved_docs:\n",
    "        metadata = doc.metadata\n",
    "        source = metadata.get('file_name', 'Unknown source')\n",
    "        page = metadata.get('page', 'Unknown page')\n",
    "        \n",
    "        context_piece = f\"\"\"\n",
    "        Source: {source} (Page {page})\n",
    "        Content: {doc.page_content}\n",
    "        \"\"\"\n",
    "        context_pieces.append(context_piece)\n",
    "    \n",
    "    return \"\\n\\n---\\n\\n\".join(context_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [rag_search]\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound_model = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: MessagesState) -> Literal[\"action\", \"end\"]:\n",
    "    \"\"\"Determine if we should continue the conversation.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if not last_message.tool_calls:\n",
    "        return END\n",
    "    return \"action\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState):\n",
    "    # Create a system prompt that includes RAG instructions\n",
    "    system_prompt = \"\"\"You are a helpful assistant with access to a document database. \n",
    "    When answering questions, use the rag_search tool to find relevant information.\n",
    "    Always provide thorough, clear explanations and cite your sources.\n",
    "    Break down complex topics into understandable parts and use examples when helpful.\n",
    "    End your responses by asking if the user would like more information.\"\"\"\n",
    "    \n",
    "    # Add system message if it's not already present\n",
    "    if not state[\"messages\"] or state[\"messages\"][0].role != \"system\":\n",
    "        state[\"messages\"].insert(0, SystemMessage(content=system_prompt))\n",
    "    \n",
    "    response = bound_model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", tool_node)\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    [\"action\", END],\n",
    ")\n",
    "workflow.add_edge(\"action\", \"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".query_func_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
