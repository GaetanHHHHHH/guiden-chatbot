{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test LangChain with Azure OpenAI deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv # type: ignore\n",
    "from langchain_openai import AzureChatOpenAI # type: ignore\n",
    "from langchain_core.prompts import ChatPromptTemplate # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_model = \"guiden-gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=oai_model,\n",
    "    api_version=\"2024-02-01\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"Loretta did nothing wrong.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"Japanese\",\n",
    "        \"input\": \"Loretta did nothing wrong.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n",
    "Goal: Adapt this tutorial to Azure: https://github.com/GaetanHHHHHH/deeplearningai-short-courses/blob/main/langchain_chat_with_your_data/notebooks/03_vectorstores_and_embeddings.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup + load doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import numpy as np\n",
    "# Move from LangChain doc processor to Document Intelligence:\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from langchain_core.documents import Document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing: Document Intelligence loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "oai_model = os.getenv(\"AZURE_DEPLOYMENT_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=oai_model,\n",
    "    api_version=\"2024-02-01\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_EMBED_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_EMBED_API_KEY\")\n",
    ")\n",
    "\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=os.getenv(\"AZURE_SEARCH_SERVICE\"),\n",
    "    azure_search_key=os.environ[\"AZURE_SEARCH_API_KEY\"],\n",
    "    index_name=\"dataroots-guidelines-vector-index\",\n",
    "    embedding_function=embeddings.embed_query,\n",
    "    additional_search_client_options={\"retry_total\": 4},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_client = DocumentAnalysisClient(\n",
    "    endpoint=os.getenv(\"AZURE_DOC_INT_ENDPOINT\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"AZURE_DOC_INT_API_KEY\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_blob_document(container_name: str, blob_name: str):\n",
    "    \"\"\"\n",
    "    Simulates the blob trigger function by processing a blob document file.\n",
    "    This helps us test our document processing logic without needing Azure Functions.\n",
    "    \"\"\"\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(\n",
    "            os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "        )\n",
    "    blob_client = blob_service_client.get_container_client(container_name).get_blob_client(blob_name)\n",
    "    blob_content = blob_client.download_blob().readall()\n",
    "    \n",
    "    # Start the document analysis - notice we're using begin_analyze_document\n",
    "    # instead of begin_analyze_document_from_url since we have a local file\n",
    "    poller = document_client.begin_analyze_document(\n",
    "        \"prebuilt-document\",\n",
    "        blob_content\n",
    "    )\n",
    "    \n",
    "    result = poller.result()   \n",
    "    print(f\"Processed document: {blob_name}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_blob_document(container_name: str, blob_name: str):\n",
    "\n",
    "    result = process_blob_document(container_name, blob_name)\n",
    "    \n",
    "    document_chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    target_chunk_size = 1000\n",
    "    \n",
    "    for paragraph in result.paragraphs:\n",
    "        paragraph_text = paragraph.content\n",
    "        \n",
    "        # If adding this paragraph would exceed our target size\n",
    "        if current_length + len(paragraph_text) > target_chunk_size and current_chunk:\n",
    "            document_chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "            \n",
    "        current_chunk.append(paragraph_text)\n",
    "        current_length += len(paragraph_text)\n",
    "    \n",
    "    if current_chunk:\n",
    "        document_chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return document_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_upload_blob_document(container_name: str, blob_name: str):\n",
    "\n",
    "    chunks = chunk_blob_document(container_name, blob_name)\n",
    "    \n",
    "    documents_to_upload = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        vector = embeddings.embed_query(chunk)\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"id\": f\"{blob_name.replace('.pdf', '')}-chunk-{i}\",\n",
    "                \"vector\": vector,\n",
    "                \"source\": blob_name,\n",
    "                \"chunk_id\": i\n",
    "            }\n",
    "        )\n",
    "        documents_to_upload.append(doc)\n",
    "        \n",
    "        if len(documents_to_upload) >= 5:\n",
    "            vector_store.add_texts(\n",
    "                texts=[doc.page_content for doc in documents_to_upload],\n",
    "                metadatas=[doc.metadata for doc in documents_to_upload]\n",
    "            )\n",
    "            documents_to_upload = []\n",
    "            \n",
    "    if documents_to_upload:\n",
    "        vector_store.add_texts(\n",
    "            texts=[doc.page_content for doc in documents_to_upload],\n",
    "            metadatas=[doc.metadata for doc in documents_to_upload]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_name = \"st-dataroots-guiden-pdfstorage\"\n",
    "test_blob_name = \"xmas_project_1.pdf\"\n",
    "extracted_text = process_blob_document(container_name, test_blob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_blob_document(container_name, test_blob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_and_upload_blob_document(container_name, test_blob_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Testing: Document Intelligence loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(\"../files/Development of a RAG-Chatbot for Rule and Guideline Retrieval.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"guiden-text-embedding-3-small\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_EMBED_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_EMBED_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"i like dogs\"\n",
    "sentence2 = \"i like canines\"\n",
    "sentence3 = \"the weather is ugly outside\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embeddings.embed_query(sentence1)\n",
    "embedding2 = embeddings.embed_query(sentence2)\n",
    "embedding3 = embeddings.embed_query(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(embedding1, embedding2), np.dot(embedding1, embedding3), np.dot(embedding2, embedding3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=os.getenv(\"AZURE_SEARCH_SERVICE\"),\n",
    "    azure_search_key=os.environ[\"AZURE_SEARCH_API_KEY\"],\n",
    "    index_name=\"dataroots-guidelines-vector-index\",\n",
    "    embedding_function=embeddings.embed_query,\n",
    "    additional_search_client_options={\"retry_total\": 4},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.add_documents(documents=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vector_store.similarity_search(\n",
    "    query=\"What is the tech stack of the project?\",\n",
    "    k=3,\n",
    "    search_type=\"hybrid\",\n",
    ")\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval & question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RetrievalQA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vector_store.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"What is the tech stack of the project?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": query})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use five sentences maximum. Keep the answer as concise as possible. Always say \"Would you like more information?\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Are on-prem technologies needed for this project?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"source_documents\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
