{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test indexing process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import uuid\n",
    "import base64\n",
    "\n",
    "import azure.functions as func\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import AzureSearch\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "print(\"Environment variables loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=os.getenv(\"AZURE_DEPLOYMENT_MODEL\"),\n",
    "    api_version=\"2024-02-01\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "document_client = DocumentAnalysisClient(\n",
    "    endpoint=os.getenv(\"AZURE_DOC_INT_ENDPOINT\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"AZURE_DOC_INT_API_KEY\"))\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_EMBED_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_EMBED_API_KEY\")\n",
    ")\n",
    "\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=os.getenv(\"AZURE_SEARCH_SERVICE\"),\n",
    "    azure_search_key=os.environ[\"AZURE_SEARCH_API_KEY\"],\n",
    "    index_name=\"dataroots-guidelines-vector-index\",\n",
    "    embedding_function=embeddings.embed_query,\n",
    "    search_options={\n",
    "        \"select\": \"id,content,metadata\",  # Specify which fields to return\n",
    "        \"vector_fields\": None  # Don't include vector fields in results\n",
    "    },\n",
    "    additional_search_client_options={\n",
    "        \"retry_total\": 4,\n",
    "        \"connection_timeout\": 5,\n",
    "        \"read_timeout\": 30\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_blob_document(container_name: str, blob_name: str):\n",
    "    \"\"\"\n",
    "    Simulates the blob trigger function by processing a blob document file.\n",
    "    This helps us test our document processing logic without needing Azure Functions.\n",
    "    \"\"\"\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(\n",
    "            os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "        )\n",
    "    blob_client = blob_service_client.get_container_client(container_name).get_blob_client(blob_name)\n",
    "    blob_content = blob_client.download_blob().readall()\n",
    "    \n",
    "    # Start the document analysis - notice we're using begin_analyze_document\n",
    "    # instead of begin_analyze_document_from_url since we have a local file\n",
    "    result = document_client.begin_analyze_document(\n",
    "        \"prebuilt-document\",\n",
    "        blob_content\n",
    "    ).result() \n",
    "    print(f\"Processed document: {blob_name}\")\n",
    "    \n",
    "    # Return the extracted text for further processing if needed\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_blob_document(container_name: str, blob_name: str):\n",
    "\n",
    "    result = process_blob_document(container_name, blob_name)\n",
    "    # Analyze the document\n",
    "    \n",
    "    document_chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    target_chunk_size = 1000\n",
    "    \n",
    "    for paragraph in result.paragraphs:\n",
    "        paragraph_text = paragraph.content\n",
    "        \n",
    "        # If adding this paragraph would exceed our target size\n",
    "        if current_length + len(paragraph_text) > target_chunk_size and current_chunk:\n",
    "            # Save the current chunk and start a new one\n",
    "            document_chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "            \n",
    "        current_chunk.append(paragraph_text)\n",
    "        current_length += len(paragraph_text)\n",
    "    \n",
    "    # add the last chunk\n",
    "    if current_chunk:\n",
    "        document_chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return document_chunks, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_upload_blob_document(container_name: str, blob_name: str):\n",
    "\n",
    "    chunks, result = chunk_blob_document(container_name, blob_name)\n",
    "    logging.info(f\"Document analysis result attributes: {dir(result)}\")\n",
    "    \n",
    "    documents_to_upload = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "\n",
    "        metadata = get_metadata(result, blob_name, i, chunks)\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        documents_to_upload.append(doc)\n",
    "        \n",
    "        if len(documents_to_upload) >= 5:\n",
    "            vector_store.add_texts(\n",
    "                texts=[doc.page_content for doc in documents_to_upload],\n",
    "                metadatas=[doc.metadata for doc in documents_to_upload]\n",
    "            )\n",
    "            documents_to_upload = []\n",
    "            \n",
    "    if documents_to_upload:\n",
    "        vector_store.add_texts(\n",
    "            texts=[doc.page_content for doc in documents_to_upload],\n",
    "            metadatas=[doc.metadata for doc in documents_to_upload]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing metadata available from form recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_name = \"st-dataroots-guiden-pdfstorage\"\n",
    "blob_names = [\"Development of a RAG-Chatbot for Rule and Guideline Retrieval.pdf\", \"xmas_project_2.pdf\", \"m1-generative-ai-engineering-with-databricks.pdf\"]\n",
    "i = 2\n",
    "blob_name = blob_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = process_blob_document(container_name, blob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in result.to_dict()['paragraphs']:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_titles = []\n",
    "\n",
    "for elem in result.to_dict()['paragraphs']:\n",
    "    if elem['role'] in ['sectionHeading', 'title', 'heading']:\n",
    "        section_titles.append(elem['content'])\n",
    "\n",
    "section_titles    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = section_titles[0]\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_type = blob_name.split('.')[-1]\n",
    "file_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(result, blob_name, i, chunks) -> dict:\n",
    "    metadata = {}\n",
    "    # Get project structure\n",
    "    metadata['section_titles'] = []\n",
    "    for elem in result.to_dict()['paragraphs']:\n",
    "        if elem['role'] in ['sectionHeading', 'title', 'heading']:\n",
    "            metadata['section_titles'].append(elem['content'])\n",
    "\n",
    "    # Add other metadata\n",
    "    metadata['title'] = metadata['section_titles'][0]\n",
    "    metadata['file_type'] = blob_name.split('.')[-1]\n",
    "    metadata['file_name'] = blob_name\n",
    "    metadata['page'] = i + 1\n",
    "    metadata['total_pages'] = len(chunks)\n",
    "\n",
    "    return(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks, result = chunk_blob_document(container_name, blob_name)\n",
    "    \n",
    "documents_to_upload = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    metadata = get_metadata(result, blob_name, i, chunks)\n",
    "    print(metadata)\n",
    "    \n",
    "    doc = Document(\n",
    "        page_content=chunk,\n",
    "        metadata=metadata\n",
    "    )\n",
    "    documents_to_upload.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents_to_upload[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
